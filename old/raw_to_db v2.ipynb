{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note to self: you can take a slice of columns like this: character.loc[:,'tconst':'character']\n",
    "# todo list\n",
    "- >>> redo the tables: rethink the whole dataset as parquet files that can have category types. If there are a lot of missing values, turn into separate table.\n",
    "- >>> use efficient int and float types by first using .convert_dtypes() to look for best type.\n",
    "- dtype category is not preserved in parquet file\n",
    "\n",
    "- is using the id parque file fster to convert id's?\n",
    "- what is faster? list to columns to stacked column (the expand technique) or explode technique?\n",
    "- not sure if all the copy's are needed\n",
    "- set dtypes (convert text to string)\n",
    "- nconst and tconst must be converted to categories\n",
    "- optimize\n",
    "- check todo's\n",
    "- we are going with parquet. sql can be used later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import gzip\n",
    "import shutil\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "START_TIME = time.time()\n",
    "\n",
    "BASE_URL = \"https://datasets.imdbws.com/\"\n",
    "\n",
    "PARQ_PATH = \"data/imdb/parquet/\"\n",
    "\n",
    "FILES_IMDB = {\n",
    "    \"tit_bas\" : \"title.basics.tsv\",\n",
    "    \"tit_rate\" : \"title.ratings.tsv\",\n",
    "    \"name_bas\" : \"name.basics.tsv\",\n",
    "    \"tit_prin\" : \"title.principals.tsv\",\n",
    "    \"cast_crew\" : \"title.crew.tsv\",\n",
    "}\n",
    "\n",
    "FILES_IMDB_PARQ = {\n",
    "    \"tit_bas\" : \"title_basics.parquet\",\n",
    "    \"genres\" : \"genres.parquet\",\n",
    "    \"tit_rate\" : \"title_ratings.parquet\",\n",
    "    'directors' : 'directors.parquet',\n",
    "    'writers' : 'writers.parquet',\n",
    "    'prim_prof' : 'primary_profession.parquet',\n",
    "    'known_for' : 'known_for_titles.parquet',\n",
    "    'name_bas' : 'name_basics.parquet',\n",
    "    'const' : 'ids.parquet',\n",
    "    'ordering' : 'ordering.parquet',\n",
    "    'character' : 'character.parquet',\n",
    "    'job' : 'job.parquet',\n",
    "    'character' : 'character.parquet'\n",
    "}\n",
    "\n",
    "FILES_HAND = {\n",
    "    \"add_seen\": \"add_movies_seen.txt\",\n",
    "    \"add_unseen\": \"add_movies_unseen.txt\",\n",
    "    \"add_secop\": \"add_movies_second_opinion.txt\",\n",
    "    \"raw_status\": \"raw_status.xlsx\"\n",
    "}\n",
    "\n",
    "FILES_GENERATED = {\n",
    "    \"films_raw\": \"films_raw.pkl\",\n",
    "    \"films_reading\": \"films_reading.xlsx\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTNconst(df, key, set_index, drop_col):\n",
    "    # tconst\n",
    "    # We remove the first two tt and convert it to int.\n",
    "    # Commands are not chained, baecause this can cause memory issues with big files.\n",
    "    df.loc[:,key] = df.loc[:,key].str.slice(2)\n",
    "    df.loc[:,key] = df.loc[:,key].astype('Int64')\n",
    "    df.loc[:,key] = df.loc[:,key].astype('category')\n",
    "    if set_index:\n",
    "        df = df.set_index(key, drop=drop_col)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make ID conversion table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertIds():\n",
    "    title_basics_path = os.path.join(\"data/imdb/\",FILES_IMDB['tit_bas'])\n",
    "    tconst = pd.read_csv(title_basics_path,sep='\\t', quotechar='\\t', low_memory=False, usecols=['tconst'])\n",
    "    tconst = tconst.replace(to_replace = \"\\\\N\", value = np.nan)\n",
    "\n",
    "    name_bas_path = os.path.join(\"data/imdb/\",FILES_IMDB['name_bas'])\n",
    "    nconst = pd.read_csv(name_bas_path,sep='\\t', low_memory=True, usecols=['nconst'])\n",
    "    nconst = nconst.replace(to_replace = \"\\\\N\", value = np.nan)\n",
    "\n",
    "    # make new columns and rename\n",
    "    tconst['stringid'] = tconst['tconst']\n",
    "    tconst = tconst.rename(columns={'tconst': \"intid\"})\n",
    "    tconst[\"type\"] = 'tconst'\n",
    "\n",
    "    nconst['stringid'] = nconst['nconst']\n",
    "    nconst = nconst.rename(columns={'nconst': \"intid\"})\n",
    "    nconst[\"type\"] = 'nconst'\n",
    "\n",
    "    # refactor id's\n",
    "    const = setTNconst(pd.concat([tconst, nconst]), 'intid', set_index=False, drop_col=False)\n",
    "\n",
    "    # set types\n",
    "    const.loc[:,'stringid'] = const.loc[:,'stringid'].astype('category')\n",
    "    const.loc[:,'type'] = const.loc[:,'type'].astype('category')\n",
    "    const.loc[:,'intid'] = const.loc[:,'intid'].astype(int)\n",
    "\n",
    "    # set index\n",
    "    const = const.set_index(\"stringid\", drop=True)\n",
    "\n",
    "    # write to disk\n",
    "    path = os.path.join(PARQ_PATH, FILES_IMDB_PARQ['const'])\n",
    "    const.to_parquet(path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertIds()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# title basics - done (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTitleBasics():\n",
    "    # read\n",
    "    title_basics_path = os.path.join(\"data/imdb/\",FILES_IMDB['tit_bas'])\n",
    "    title_basics = pd.read_csv(title_basics_path,sep='\\t', quotechar='\\t', low_memory=False)\n",
    "    title_basics = title_basics.replace(to_replace = \"\\\\N\", value = np.nan)\n",
    "\n",
    "    # convert and set tconst as index\n",
    "    title_basics = setTNconst(title_basics, 'tconst', set_index=True, drop_col=True)\n",
    "\n",
    "    # convert isAdult\n",
    "    # Those movies are checked, and were not adult movies.\n",
    "    # movies that are not an 1 or '1' aren't adult movies\n",
    "    title_basics.loc[:,\"isAdult\"] = title_basics.loc[:,\"isAdult\"].fillna(0).astype('str')\n",
    "    title_basics.loc[title_basics.loc[:,'isAdult'] != '1', 'isAdult'] = '0'\n",
    "    title_basics.loc[:,\"isAdult\"] = title_basics.loc[:,\"isAdult\"].map({'1': True, '0': False})\n",
    "\n",
    "    # auto convert\n",
    "    title_basics = title_basics.convert_dtypes()\n",
    "\n",
    "    # titleType\n",
    "    # There are a few null values (np.nan)\n",
    "    # convert to category\n",
    "    title_basics.loc[:,\"titleType\"] = title_basics.loc[:,\"titleType\"].astype('category')\n",
    "\n",
    "    # startYear & endYear & runtimeMinutes\n",
    "    # both contain a lot of np.nan\n",
    "    title_basics.loc[:,[\"startYear\",\"endYear\",\"runtimeMinutes\"]] = title_basics.loc[:,[\"startYear\",\"endYear\",\"runtimeMinutes\"]].astype('Int16')\n",
    "\n",
    "    # make genres table\n",
    "    genres = (title_basics\n",
    "              .genres.str.split(\",\", expand=True)\n",
    "              .stack()\n",
    "              .reset_index()\n",
    "              .rename(columns={\"level_0\":'tconst', 0:\"genre\"})\n",
    "              .set_index(\"tconst\")\n",
    "              .drop(columns='level_1'))\n",
    "    genres['genre'] = genres['genre'].astype('category')\n",
    "\n",
    "    # drop genre\n",
    "    title_basics = title_basics.drop(columns='genres')\n",
    "\n",
    "    # write to disk\n",
    "    path = os.path.join(PARQ_PATH, FILES_IMDB_PARQ['tit_bas'])\n",
    "    title_basics.to_parquet(path, engine='pyarrow')\n",
    "    path = os.path.join(PARQ_PATH, FILES_IMDB_PARQ['genres'])\n",
    "    genres.to_parquet(path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertTitleBasics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# title rate - done (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTitleRate():\n",
    "    # read\n",
    "    title_rate_path = os.path.join(\"data/imdb/\",FILES_IMDB['tit_rate'])\n",
    "    title_rate = pd.read_csv(title_rate_path,sep='\\t', quotechar='\\t', low_memory=False)\n",
    "    title_rate = title_rate.replace(to_replace = \"\\\\N\", value = np.nan)\n",
    "\n",
    "    # convert and set tconst as index\n",
    "    title_rate = setTNconst(title_rate, 'tconst', set_index=True, drop_col=True)\n",
    "\n",
    "    # convert averageRating\n",
    "    title_rate.loc[:,'averageRating'] = title_rate.loc[:,'averageRating'].astype('float32')\n",
    "\n",
    "    # convert numVotes\n",
    "    title_rate.loc[:,'numVotes'] = title_rate.loc[:,'numVotes'].astype('uint32')\n",
    "\n",
    "    # write to disk\n",
    "    path = os.path.join(PARQ_PATH, FILES_IMDB_PARQ['tit_rate'])\n",
    "    title_rate.to_parquet(path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertTitleRate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## title crew - done (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTitleCrew():\n",
    "    # read directors and writers file\n",
    "    title_crew_path = os.path.join(\"data/imdb/\",FILES_IMDB['cast_crew'])\n",
    "    title_crew = pd.read_csv(title_crew_path,sep='\\t', quotechar='\\t', low_memory=True)\n",
    "    title_crew = title_crew.replace(to_replace = \"\\\\N\", value = np.nan)\n",
    "\n",
    "    # set index\n",
    "    title_crew = setTNconst(title_crew, 'tconst', set_index=True, drop_col=True)\n",
    "\n",
    "    for crew in ['directors', 'writers']:\n",
    "        # make  dataframe\n",
    "        crew_frame = (title_crew.loc[:,crew]\n",
    "                    .dropna()\n",
    "                    .str.split(\",\")\n",
    "                    .to_frame()\n",
    "                    .explode(crew))\n",
    "        \n",
    "        # convert\n",
    "        crew_frame = setTNconst(crew_frame, crew, set_index=False, drop_col=False)\n",
    "        crew_frame.loc[:,crew] = crew_frame.loc[:,crew].astype('category').to_frame()\n",
    "\n",
    "        # write to disk\n",
    "        path = os.path.join(PARQ_PATH, FILES_IMDB_PARQ[crew])\n",
    "        crew_frame.to_parquet(path, engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertTitleCrew()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## name basics - done (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertNameBasics():\n",
    "    # load data\n",
    "    name_bas_path = os.path.join(\"data/imdb/\",FILES_IMDB['name_bas'])\n",
    "    name_bas = pd.read_csv(name_bas_path,sep='\\t', low_memory=True)\n",
    "    name_bas = name_bas.replace(to_replace = \"\\\\N\", value = np.nan)\n",
    "\n",
    "    # convert\n",
    "    name_bas.loc[:,[\"nconst\",\"primaryName\",\"primaryProfession\",\"knownForTitles\"]] = name_bas.loc[:,[\"nconst\",\"primaryName\",\"primaryProfession\",\"knownForTitles\"]].astype('string')\n",
    "    name_bas.loc[:,[\"birthYear\",\"deathYear\"]] = name_bas.loc[:,[\"birthYear\",\"deathYear\"]].astype('Int16')\n",
    "    name_bas = setTNconst(name_bas, 'nconst', set_index=True, drop_col=True)\n",
    "    name_convert = {'primaryProfession': 'prim_prof', 'knownForTitles': \"known_for\"}\n",
    "    \n",
    "    for info in ['primaryProfession', 'knownForTitles']:\n",
    "        # get column and delete from original\n",
    "        data = name_bas[info].dropna().to_frame()\n",
    "        name_bas = name_bas.drop(columns=info)\n",
    "\n",
    "        # explode data\n",
    "        data = (data.loc[:,info]\n",
    "             .str.split(',')\n",
    "             .to_frame()\n",
    "             .explode(info))\n",
    "        \n",
    "        # convert\n",
    "        if info == 'primaryProfession':\n",
    "            data.loc[:,info] = data.loc[:,info].astype('string').astype(\"category\")\n",
    "        else:\n",
    "            data = setTNconst(data, info, set_index=False, drop_col=False)\n",
    "        \n",
    "        # write to disk\n",
    "        path = os.path.join(PARQ_PATH, FILES_IMDB_PARQ[name_convert[info]])\n",
    "        data.to_parquet(path, engine='pyarrow')\n",
    "    \n",
    "    # write to disk\n",
    "    path = os.path.join(PARQ_PATH, FILES_IMDB_PARQ['name_bas'])\n",
    "    name_bas.to_parquet(path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertNameBasics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert title principals - test these functions in the real script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertOrdering():\n",
    "    # set vars\n",
    "    chunk_size = int(55996061 / 2)\n",
    "    tit_prin_path = os.path.join(\"data/imdb/download/\", FILES_IMDB['tit_prin'])\n",
    "    ordering = pd.DataFrame(columns=['tconst','nconst','ordering','category'])\n",
    "    ordering = ordering.set_index('tconst')\n",
    "\n",
    "    # load file using chunks\n",
    "    for chunk in pd.read_csv(tit_prin_path,sep='\\t', low_memory=True, usecols=['tconst', 'nconst', 'ordering', 'category'], chunksize=chunk_size):\n",
    "        chunk = chunk.replace(to_replace = \"\\\\N\", value = np.nan)\n",
    "\n",
    "        # set dtypes, convert id's and set index\n",
    "        chunk.loc[:,'ordering'] = chunk.loc[:,'ordering'].astype(int)\n",
    "        chunk = setTNconst(chunk, 'tconst', set_index=True, drop_col=True)\n",
    "        chunk = setTNconst(chunk, 'nconst', set_index=False, drop_col=False)\n",
    "\n",
    "        # merge chunks\n",
    "        ordering = pd.concat([ordering, chunk]) \n",
    "    \n",
    "    ordering.loc[:,'category'] = ordering.loc[:,'category'].astype(str).astype('category')\n",
    "\n",
    "    # write to disk\n",
    "    path = os.path.join(PARQ_PATH, FILES_IMDB_PARQ['ordering'])\n",
    "    ordering.to_parquet(path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertJob():\n",
    "    # set vars\n",
    "    chunk_size = int(55996061 / 2)\n",
    "    tit_prin_path = os.path.join(\"data/imdb/download/\", FILES_IMDB['tit_prin'])\n",
    "    job = pd.DataFrame(columns=['tconst', 'nconst', 'job'])\n",
    "    job = job.set_index('tconst')\n",
    "\n",
    "    \n",
    "    # load file using chunks\n",
    "    for chunk in pd.read_csv(tit_prin_path,sep='\\t', low_memory=True, usecols=['tconst', 'nconst', 'job'], chunksize=chunk_size):\n",
    "        chunk = chunk.replace(to_replace = \"\\\\N\", value = np.nan).dropna()\n",
    "\n",
    "        # set dtypes, convert id's and set index\n",
    "        chunk = setTNconst(chunk, 'tconst', set_index=True, drop_col=True)\n",
    "        chunk = setTNconst(chunk, 'nconst', set_index=False, drop_col=False)\n",
    "        chunk.loc[:,'job'] = chunk.loc[:,'job'].astype('category')\n",
    "\n",
    "        # merge chunks\n",
    "        job = pd.concat([job, chunk]) \n",
    "\n",
    "    # set job as category\n",
    "    job.loc[:,'job'] = job.loc[:,'job'].astype(str).astype('category')\n",
    "\n",
    "    # write to disk\n",
    "    path = os.path.join(PARQ_PATH, FILES_IMDB_PARQ['job'])\n",
    "    job.to_parquet(path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCharacter():\n",
    "    # set vars\n",
    "    chunk_size = int(55996061 / 2)\n",
    "    tit_prin_path = os.path.join(\"data/imdb/download/\", FILES_IMDB['tit_prin'])\n",
    "    character = pd.DataFrame(columns=['tconst', 'nconst', 'characters'])\n",
    "    character = character.set_index('tconst')\n",
    "\n",
    "    # load file using chunks\n",
    "    for chunk in pd.read_csv(tit_prin_path,sep='\\t', low_memory=True, usecols=['tconst', 'nconst', 'characters'], chunksize=chunk_size):\n",
    "        chunk = chunk.replace(to_replace = \"\\\\N\", value = np.nan).dropna()\n",
    "\n",
    "        # set dtypes, convert id's and set index\n",
    "        chunk = setTNconst(chunk, 'tconst', set_index=True, drop_col=True)\n",
    "        chunk = setTNconst(chunk, 'nconst', set_index=False, drop_col=False)\n",
    "\n",
    "        # explode the list\n",
    "        chunk.loc[:,\"characters\"] = chunk.loc[:,\"characters\"].apply(eval)\n",
    "        chunk = chunk.explode('characters')\n",
    "\n",
    "        # merge chunks\n",
    "        character = pd.concat([character, chunk])\n",
    "\n",
    "    # set characters as catagory\n",
    "    chunk.loc[:,\"characters\"] = chunk.loc[:,\"characters\"].astype(str).astype('category')\n",
    "\n",
    "    # write to disk\n",
    "    path = os.path.join(PARQ_PATH, FILES_IMDB_PARQ['character'])\n",
    "    character.to_parquet(path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/ManualInstalls/anaconda3/envs/ML/lib/python3.10/site-packages/pandas/core/indexes/category.py:586: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  return Index(res, name=name)\n"
     ]
    }
   ],
   "source": [
    "convertOrdering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/ManualInstalls/anaconda3/envs/ML/lib/python3.10/site-packages/pandas/core/indexes/category.py:586: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  return Index(res, name=name)\n"
     ]
    }
   ],
   "source": [
    "convertCharacter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/ManualInstalls/anaconda3/envs/ML/lib/python3.10/site-packages/pandas/core/indexes/category.py:586: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  return Index(res, name=name)\n"
     ]
    }
   ],
   "source": [
    "convertJob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 0:16:31.203917\n"
     ]
    }
   ],
   "source": [
    "END_TIME = time.time()\n",
    "print(\"total time:\",str(timedelta(seconds=END_TIME-START_TIME)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a test to find out why directors and writers are put in the wrong folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import gzip\n",
    "import shutil\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "START_TIME = time.time()\n",
    "\n",
    "BASE_URL = \"https://datasets.imdbws.com/\"\n",
    "PARQ_PATH = \"data/imdb/parquet/\"\n",
    "DOWNLOAD_PATH = \"data/imdb/download/\"\n",
    "\n",
    "\n",
    "FILES_IMDB = {\n",
    "    # \"tit_bas\" : \"title.basics.tsv\",\n",
    "    # \"tit_rate\" : \"title.ratings.tsv\",\n",
    "    # \"name_bas\" : \"name.basics.tsv\",\n",
    "    # \"tit_prin\" : \"title.principals.tsv\",\n",
    "    \"cast_crew\" : \"title.crew.tsv\",\n",
    "}\n",
    "\n",
    "FILES_IMDB_PARQ = {\n",
    "    \"tit_bas\" : \"title_basics.parquet\",\n",
    "    \"genres\" : \"genres.parquet\",\n",
    "    \"tit_rate\" : \"title_ratings.parquet\",\n",
    "    'directors' : 'directors.parquet',\n",
    "    'writers' : 'writers.parquet',\n",
    "    'prim_prof' : 'primary_profession.parquet',\n",
    "    'known_for' : 'known_for_titles.parquet',\n",
    "    'name_bas' : 'name_basics.parquet',\n",
    "    'const' : 'ids.parquet',\n",
    "    'ordering' : 'ordering.parquet',\n",
    "    'character' : 'character.parquet',\n",
    "    'job' : 'job.parquet',\n",
    "}\n",
    "\n",
    "FILES_HAND = {\n",
    "    \"add_seen\": \"add_movies_seen.txt\",\n",
    "    \"add_unseen\": \"add_movies_unseen.txt\",\n",
    "    \"add_secop\": \"add_movies_second_opinion.txt\",\n",
    "    \"raw_status\": \"raw_status.xlsx\"\n",
    "}\n",
    "\n",
    "FILES_GENERATED = {\n",
    "    \"films_raw\": \"films_raw.pkl\",\n",
    "    \"films_reading\": \"films_reading.xlsx\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTNconst(df, key, set_index, drop_col):\n",
    "    # helper function to convert id's to integers\n",
    "    # tconst\n",
    "    # We remove the first two tt and convert it to int.\n",
    "    # Commands are not chained, baecause this can cause memory issues with big files.\n",
    "    df.loc[:,key] = df.loc[:,key].str.slice(2)\n",
    "    df.loc[:,key] = df.loc[:,key].astype('Int64')\n",
    "    df.loc[:,key] = df.loc[:,key].astype('category')\n",
    "    if set_index:\n",
    "        df = df.set_index(key, drop=drop_col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTitleCrew():\n",
    "    # read directors and writers file\n",
    "    title_crew_path = os.path.join(DOWNLOAD_PATH,FILES_IMDB['cast_crew'])\n",
    "    title_crew = pd.read_csv(title_crew_path,sep='\\t', quotechar='\\t', low_memory=True)\n",
    "    title_crew = title_crew.replace(to_replace = \"\\\\N\", value = np.nan)\n",
    "\n",
    "    # set index\n",
    "    title_crew = setTNconst(title_crew, 'tconst', set_index=True, drop_col=True)\n",
    "\n",
    "    for crew in ['directors', 'writers']:\n",
    "        # make  dataframe\n",
    "        crew_frame = (title_crew.loc[:,crew]\n",
    "                    .dropna()\n",
    "                    .str.split(\",\")\n",
    "                    .to_frame()\n",
    "                    .explode(crew))\n",
    "        \n",
    "        # convert\n",
    "        crew_frame = setTNconst(crew_frame, crew, set_index=False, drop_col=False)\n",
    "        crew_frame.loc[:,crew] = crew_frame.loc[:,crew].astype('category').to_frame()\n",
    "\n",
    "        # write to disk\n",
    "        path = os.path.join(PARQ_PATH, FILES_IMDB_PARQ[crew])\n",
    "        print(path)\n",
    "        crew_frame.to_parquet(path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # \"\"\" Downloads needed files and removes old files if they already exist. \"\"\"\n",
    "    END_TIME = time.time()\n",
    "    TEMP_START_TIME = time.time()\n",
    "\n",
    "    # if not os.path.exists(\"data/imdb/download/\"):\n",
    "    #     os.makedirs(\"data/imdb/download\")\n",
    "    # if not os.path.exists(\"data/imdb/parquet/\"):\n",
    "    #     os.makedirs(\"data/imdb/parquet\")\n",
    "    \n",
    "    # # remove old files\n",
    "    # for file in FILES_IMDB_PARQ.values():\n",
    "    #     file_name = os.path.join(PARQ_PATH,file)\n",
    "    #     if os.path.exists(file_name):\n",
    "    #         os.remove(file_name)\n",
    "    \n",
    "    # make dict with functions to execute\n",
    "    # funcs = {\n",
    "    #     # \"tit_bas\" : [ convertTitleBasics ],\n",
    "    #     # \"tit_rate\" : [ convertTitleRate ],\n",
    "    #     # \"name_bas\" : [ convertNameBasics, convertIds ],\n",
    "    #     # \"tit_prin\" : [ convertOrdering, convertCharacter, convertJob ],\n",
    "    #     \"cast_crew\" : [ convertTitleCrew ],\n",
    "    #     }\n",
    "\n",
    "    #  download and prepare files\n",
    "    for file_key, file_value in FILES_IMDB.items():\n",
    "        TEMP_START_TIME = time.time()\n",
    "        print(\"file: \",file_value)\n",
    "\n",
    "        file_name = os.path.join(DOWNLOAD_PATH,file_value)\n",
    "        file_zip = file_name+\".gz\"\n",
    "        file_url = BASE_URL+file_value+\".gz\"\n",
    "\n",
    "        # download file\n",
    "        response = requests.get(file_url)\n",
    "        open(file_zip , \"wb\").write(response.content)\n",
    "\n",
    "        # unzip file\n",
    "        with gzip.open(file_zip, 'rb') as f_in:\n",
    "            with open(file_name, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "    convertTitleCrew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  title.crew.tsv\n",
      "data/imdb/parquet/directors.parquet\n",
      "data/imdb/parquet/writers.parquet\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
